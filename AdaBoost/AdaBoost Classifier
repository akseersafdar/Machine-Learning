Gradiant Boosting

Gradient boosting is a machine learning technique for regression and classification problems, 
which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. 
It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing
optimization of an arbitrary differentiable loss function.

More details @ https://en.wikipedia.org/wiki/Gradient_boosting

AdaBoost Algorithm

AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund
and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many 
other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners')
is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense 
that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. 
AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem 
than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly
better than random guessing, the final model can be proven to converge to a strong learner.

More details @

https://en.wikipedia.org/wiki/AdaBoost

In this lab, you’ll explore the breast cancer dataset and try to train the model to predict if the person is having breast
cancer or not. We will start off with a weak learner, a decision tree with maximum depth = 2.

We will then build an adaboost ensemble with 50 trees with a step of 3 and compare the performance with the weak learner.

Please Refer to the attached ipython notebook for more details. 

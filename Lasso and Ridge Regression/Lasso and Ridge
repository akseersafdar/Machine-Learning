Ridge Regression
Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems.
In statistics, the method is known as ridge regression, in machine learning it is known as weight decay, and with multiple 
independent discoveries, it is also variously known as the Tikhonov–Miller method, the Phillips–Twomey method, 
the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg–Marquardt
algorithm for non-linear least-squares problems.

More details @

https://en.wikipedia.org/wiki/Tikhonov_regularization

Lasso Regression
In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is 
a regression analysis method that performs both variable selection and regularization in order to enhance the prediction
accuracy and interpretability of the statistical model it produces.

Lasso was originally formulated for least squares models and this simple case reveals a substantial amount about the
behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections 
between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression)
the coefficient estimates need not be unique if covariates are collinear.

Though originally defined for least squares, lasso regularization is easily extended to a wide variety of statistical
models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, 
in a straightforward fashion.Lasso’s ability to perform subset selection relies on the form of the constraint and has a variety 
of interpretations including in terms of geometry, Bayesian statistics, and convex analysis.

More details @

https://en.wikipedia.org/wiki/Lasso_(statistics)
